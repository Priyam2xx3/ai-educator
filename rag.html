<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Retrieval-Augmented Generation (RAG) | AI Educator</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="app-container">
        <!-- Navbar -->
        <nav class="navbar">
            <div class="logo">‚ú¶ AI<span>Educator</span></div>
            <div class="nav-actions">
                <a href="index.html" class="nav-link">Home</a>
                <a href="roadmap.html" class="btn btn-outline" style="padding: 0.5rem 1rem;">Roadmap</a>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">üåô</button>
                <button id="mobile-menu-btn" class="mobile-menu-btn">‚ò∞</button>
            </div>
        </nav>

        <!-- Main Layout -->
        <div class="main-content-layout">
            <!-- Sidebar -->
            <aside id="sidebar" class="sidebar">
                <div class="nav-links">
                    <a href="index.html">üè† Home</a>
                    <a href="roadmap.html">üìç Roadmap</a>
                    <a href="ml.html">üìò Machine Learning</a>
                    <a href="dl.html">üìó Deep Learning</a>
                    <a href="transformers.html">üìï Transformers</a>
                    <a href="llm.html">üìô LLMs</a>
                    <a href="rag.html" class="active">üìí RAG</a>
                    <a href="agents.html">üìì AI Agents</a>
                    <a href="algorithms.html">üìë Algorithms</a>
                    <a href="references.html">üìé References</a>
                </div>
            </aside>

            <!-- Page Content -->
            <main class="content-area">
                <h1 class="page-header delay-1">Retrieval-Augmented Generation</h1>

                <section class="glass delay-1" style="padding: 2rem; margin-bottom: 2rem;">
                    <h2>What is RAG?</h2>
                    <p>RAG is an AI framework that improves the quality of LLM-generated responses by grounding the
                        model on external sources of knowledge. Instead of hallucinating answers based solely on its
                        training data, the LLM first retrieves relevant documents and uses them to formulate an accurate
                        answer.</p>
                </section>

                <div class="accordion delay-2">
                    <div class="accordion-item">
                        <div class="accordion-header">
                            RAG Architecture Pipeline
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content" style="text-align: center;">
                            <p style="margin-top: 1rem; text-align: left;">The pipeline involves three main components:
                                Ingestion, Retrieval, and Generation.</p>

                            <!-- Placeholder for Architecture Diagram -->
                            <div
                                style="background: var(--card-border); padding: 3rem; margin: 1.5rem 0; border-radius: 0.5rem; border: 2px dashed var(--primary); color: var(--text-muted); font-weight: bold; font-family: monospace;">
                                [ DIAGRAM: Document -> Chunks -> Embedding Model -> Vector DB ]<br>
                                [ DIAGRAM: User Query -> Embedding Model -> Similarity Search -> Top-K Context -> LLM
                                Prompt -> Answer ]
                            </div>

                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Embeddings
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>What they are:</strong> A mathematical representation of text, images, or
                                    audio in high-dimensional space. Words or sentences with similar meanings will have
                                    vectors that are closer together.</li>
                                <li><strong>Chunking:</strong> Large documents (like a PDF) are too big to embed whole.
                                    They are split into smaller chunks (e.g., 500 tokens) using naive splitting or
                                    semantic chunking.</li>
                                <li><strong>Models:</strong> OpenAI's text-embedding-ada-002, BAAI's bge-large-en,
                                    Cohere's embed-english-v3.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Vector Databases
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>What they do:</strong> Store the high-dimensional vectors (Embeddings)
                                    alongside their original text chunks to allow for lightning-fast similarity searches
                                    (Nearest Neighbor search).</li>
                                <li><strong>Algorithms:</strong> Approximate Nearest Neighbors (ANN), HNSW (Hierarchical
                                    Navigable Small World).</li>
                                <li><strong>Examples:</strong> Pinecone, Milvus, Qdrant, ChromaDB, Weaviate, pgvector
                                    (PostgreSQL).</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Advanced Retrieval Techniques
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Hybrid Search:</strong> Combining Vector Search (semantic meaning) with
                                    Keyword Search (BM25 sparse vectors) for highly accurate retrieval of specific names
                                    alongside concepts.</li>
                                <li><strong>Re-ranking:</strong> Retrieving Top-50 documents quickly with vector search,
                                    then using a Cross-Encoder model (like Cohere Rerank) to perfectly sort the Top-5
                                    based on relevance to the query.</li>
                                <li><strong>Query Expansion:</strong> Using an LLM to rewrite the user's short query
                                    into multiple possible queries or hypothetical document answers to improve recall.
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Deep Dive Q&A (A Curious Student's Treasure)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li>
                                    <strong>Q: Why use RAG instead of just Fine-Tuning the LLM on my company's
                                        data?</strong><br>
                                    <span class="text-muted">A: Fine-tuning is like teaching someone how to speak like a
                                        lawyer (tone/style). RAG is like giving that lawyer an open book during a test.
                                        Fine-tuning to memorize facts is expensive, hard to update (you have to re-train
                                        if a fact changes), and doesn't solve hallucination. RAG lets you instantly
                                        update knowledge just by adding a new PDF to the database, and forces the LLM to
                                        cite its sources, drastically reducing hallucinations.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: If an LLM has a 1 Million token Context Window, why do we need Vector
                                        Databases? Why not just paste all the PDFs into the prompt?</strong><br>
                                    <span class="text-muted">A: 1. Cost: API providers charge per token. Sending a
                                        million tokens for every single question gets incredibly expensive. 2. Speed:
                                        Processing 1M tokens takes a lot of time (latency). 3. "Lost in the Middle":
                                        Studies show LLMs struggle to find specific facts buried in the middle of
                                        massive prompts. Vector DBs act as a rapid filter, pulling only the 3 most
                                        relevant pages so the LLM answers quickly, cheaply, and accurately.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: What is an "Embedding" in simple terms?</strong><br>
                                    <span class="text-muted">A: Imagine mapping every concept in the universe onto a
                                        massive 3D grid. "Dog" and "Cat" would be placed very close together. "Dog" and
                                        "Carburetor" would be far apart. A neural network embedding model calculates
                                        these exact coordinates (often in 1536 dimensions, not just 3) so a computer can
                                        mathematically measure the "distance" (Cosine Similarity) between two
                                        ideas.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: What causes a RAG system to fail or give bad answers?</strong><br>
                                    <span class="text-muted">A: Usually, it's terrible "Chunking". If you naively split
                                        a PDF exactly every 500 characters, you might cut a crucial sentence in half.
                                        When the user asks a question, the Vector search pulls up half a thought, and
                                        the LLM can't understand it. Semantic chunking (making sure chunks break at
                                        logical paragraphs or sections) is critical to RAG success.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: When would Vector Search fail, but traditional Keyword Search
                                        succeed?</strong><br>
                                    <span class="text-muted">A: Vector search is great for concepts. But if you search
                                        for an exact serial number like `XJ-92A`, vector embeddings might struggle
                                        because the alphanumeric code doesn't have deep "semantic meaning". Traditional
                                        keyword search (like BM25) excels here. This is why enterprise systems use
                                        "Hybrid Search"‚Äîrunning both algorithms simultaneously and blending the
                                        results.</span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </main>
        </div>

        <button id="backToTop" title="Go to top">‚Üë</button>
    </div>

    <script src="script.js"></script>
</body>

</html>