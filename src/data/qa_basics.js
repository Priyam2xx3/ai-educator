export const mlQuestions = [
    { question: "What is the difference between Supervised and Unsupervised Learning?", answer: "Supervised learning algorithms are trained on labeled datasets, meaning the input data comes with the correct output or 'answer key' (e.g., predicting house prices based on historical data). Unsupervised learning deals with unlabeled data; the algorithm must find hidden structures or patterns on its own (e.g., grouping customers into clusters based on purchasing behavior)." },
    { question: "Explain the Bias-Variance Tradeoff.", answer: "This is the fundamental tension in ML. <strong>Bias</strong> is error from overly simplistic assumptions (Underfitting: assuming a straight line fits a curve). <strong>Variance</strong> is error from hypersensitivity to small fluctuations in training data (Overfitting: connecting every single dot perfectly but failing on new dots). The goal is to find the sweet spot that minimizes both, leading to good generalization on unseen data." },
    { question: "What is the difference between Classification and Regression?", answer: "Both are Supervised learning tasks. <strong>Classification</strong> predicts discrete, categorical labels (e.g., 'Spam' or 'Not Spam', 'Dog' or 'Cat'). <strong>Regression</strong> predicts continuous, numerical values (e.g., predicting a stock price or the exact temperature tomorrow)." },
    { question: "How do you handle missing data in a dataset?", answer: "Several ways depending on context: \n1. <strong>Deletion</strong>: Drop rows or columns if the missing data is minimal.\n2. <strong>Imputation</strong>: Fill in missing values using the mean, median, or mode of that column.\n3. <strong>Predictive Imputation</strong>: Use another ML model (like KNN) to predict the missing value.\n4. <strong>Indicator Variables</strong>: Create a new binary column indicating whether the value was missing or not." },
    { question: "What is Overfitting, and how can you prevent it?", answer: "Overfitting occurs when a model memorizes the training data (including its noise) rather than learning the underlying pattern, leading to poor performance on test data. Prevention methods include:\n- Getting more training data\n- Cross-validation\n- Early stopping\n- Regularization (L1/L2 penalties)\n- Reducing model complexity (e.g., shallower decision trees)" },
    { question: "Explain K-Fold Cross-Validation.", answer: "Instead of just holding out one single chunk of data for testing, K-Fold splits the dataset into 'K' equal parts (folds). The model is trained on K-1 folds and tested on the remaining 1 fold. This process is repeated K times so that every fold gets to act as the test set once. The final performance is the average of all K tests, ensuring the model's evaluation isn't just lucky or unlucky based on a single random split." },
    { question: "What is a Random Forest? Why is it better than a single Decision Tree?", answer: "A Random Forest is an 'ensemble' method. A single Decision Tree is highly prone to overfitting (it builds deep, complex rules specific to the training data). A Random Forest trains hundreds of shallow, inherently flawed decision trees on random subsets of data and features. It then averages their predictions together. The individual flaws cancel out, leaving a highly accurate and generalized prediction." },
    { question: "What is the difference between L1 (Lasso) and L2 (Ridge) Regularization?", answer: "Both add a mathematical penalty to the loss function to prevent overfitting. \n- <strong>L1 (Lasso)</strong> penalizes the absolute value of the weights. It can shrink weights exactly to zero, acting as a built-in feature selector (removing useless features entirely).\n- <strong>L2 (Ridge)</strong> penalizes the squared value of the weights. It forces weights to be very small, but rarely zero. Better when all features have some slight relevance." },
    { question: "Explain the concept of 'Ensemble Learning'.", answer: "Ensemble learning combines the predictions of multiple distinct ML models to produce a single, superior prediction. It relies on the 'wisdom of the crowd'—if independent models make uncorrelated errors, their average will be closer to the true value. Major types include Bagging (Random Forest), Boosting (XGBoost), and Stacking." },
    { question: "What is PCA (Principal Component Analysis)?", answer: "PCA is an unsupervised dimensionality reduction technique. If you have a dataset with 100 features, PCA uses linear algebra to compress those into, say, 10 'Principal Components' that retain 95% of the original variance (information). It is used to speed up training, reduce noise, and allow for 2D/3D visualization of complex data." },
    { question: "What is the 'Curse of Dimensionality'?", answer: "As the number of features (dimensions) in a dataset increases, the volume of the mathematical space grows exponentially. Data points become incredibly sparse and far apart. Distance-based algorithms (like KNN) stop working because the 'distance' to all points becomes roughly the same, making similarities meaningless. Solution: Dimensionality reduction like PCA." },
    { question: "How does the K-Nearest Neighbors (KNN) algorithm work?", answer: "KNN is a lazy learning algorithm used for classification or regression. When given a new, unseen data point, it calculates the mathematical distance (usually Euclidean) between that point and all other points in the training set. It finds the 'K' closest points mathematically and assigns the new point the majority label (or average value) of those K neighbors." },
    { question: "What is a Support Vector Machine (SVM)?", answer: "SVM is a powerful classification algorithm that seeks to find the 'hyperplane' (a mathematical line or boundary) that cleanly separates two classes of data while maximizing the 'margin' (the distance between the line and the closest data points of either class, known as support vectors). If data isn't linearly separable, SVM uses the 'Kernel Trick' to project data into higher dimensions where it can be sliced." },
    { question: "Explain the 'Kernel Trick' in SVMs.", answer: "When dealing with non-linear data (e.g., a circle of red dots completely surrounded by blue dots), a straight line cannot separate them. The Kernel Trick uses a mathematical function (like RBF or Polynomial) to instantly map the 2D points into a 3D space (adding a Z-axis, like making the red dots 'pop up' out of the screen), allowing a flat 2D plane to slice between them in 3D." },
    { question: "Why do we need to Scale or Normalize features before training?", answer: "Many algorithms (like KNN, SVM, or Neural Networks) calculate distance or use gradient descent. If Feature A is 'Income' (ranging from $20k to $150k) and Feature B is 'Age' (ranging from 18 to 80), the massive numbers in Income will mathematically overwhelm Age, making the algorithm think Income is 1,000x more important just because the digits are larger. Scaling forces all features into the same range (e.g., 0 to 1)." },
    { question: "What is the difference between a Generative and a Discriminative model?", answer: "<strong>Discriminative models</strong> (like Logistic Regression or Random Forests) draw boundaries in the data space. They learn the difference <em>between</em> classes to predict the label given an input: <code>P(Y|X)</code>.\n<strong>Generative models</strong> (like Naive Bayes or GANs) learn how the data was generated in the first place. They model the entire distribution of the classes: <code>P(X,Y)</code>, allowing them to instantly generate <em>new</em>, fake data that looks real." },
    { question: "What is Naive Bayes, and why is it 'Naive'?", answer: "It is a probabilistic classifier based on Bayes' Theorem. It is called 'Naive' because it makes a massive, usually incorrect assumption: it assumes that every single feature in the dataset is completely independent of every other feature. Despite this flawed assumption, it performs incredibly well and extremely fast, especially in text classification (like spam filtering)." },
    { question: "What is SMOTE, and when would you use it?", answer: "Synthetic Minority Over-sampling Technique. It is used when dealing with highly imbalanced datasets (e.g., 99% normal transactions, 1% fraud). You cannot just train a model on this, or it will just guess 'normal' every time and get 99% accuracy. SMOTE mathematically synthesizes entirely new, realistic examples of the minority class (fraud) to balance the training data without simply duplicating the exact same rows." },
    { question: "What is Gradient Boosting (e.g., XGBoost)?", answer: "Unlike Random Forests which build hundreds of trees independently, Gradient Boosting builds trees <em>sequentially</em>. Tree 1 makes a prediction and has some errors (residuals). Tree 2 is explicitly trained to <em>predict and fix the errors</em> made by Tree 1. Tree 3 fixes Tree 2, and so on. This aggressive focus on correcting past mistakes makes Boosting one of the most accurate tabular algorithms in existence." },
    { question: "What is the difference between Normalization (Min-Max Scaling) and Standardization (Z-Score)?", answer: "<strong>Normalization</strong> squashes all data into a hard boundary, usually exactly between 0 and 1. Highly sensitive to extreme outliers.\n<strong>Standardization</strong> reshapes the data to have a mean of 0 and a standard deviation of 1. It does not have hard bounding limits, making it far superior when dealing with data that has significant outliers." },
    { question: "How does K-Means Clustering work?", answer: "An Unsupervised algorithm. \n1. Randomly place 'K' centroids (center points) on the data map.\n2. Assign every data point to the centroid it is physically closest to.\n3. Recalculate and move the centroid to the dead-center of all the points assigned to it.\n4. Repeat steps 2 and 3 until the centroids stop moving. You now have K clusters." }
];

export const dlQuestions = [
    { question: "What is an Artificial Neural Network?", answer: "A computing system inspired by the biological brain. It consists of interconnected layers of artificial 'neurons' (nodes). Each connection has a 'weight' indicating its importance. Input data passes through these weighted connections and non-linear activation functions to produce a prediction." },
    { question: "Explain the concept of 'Backpropagation'.", answer: "The core engine of learning in Deep Learning. Once a network makes a prediction, it calculates the error (Loss). Backpropagation uses calculus (specifically the Chain Rule) to trace that error backward from the output layer through all hidden layers down to the input, calculating the gradient (slope) for every single weight. This tells the optimizer exactly how to adjust every weight to make the error slightly smaller next time." },
    { question: "What is an Activation Function, and why is it absolutely necessary?", answer: "An activation function (like ReLU, Sigmoid, or Tanh) is a mathematical gate applied to the output of a neuron. It introduces <strong>non-linearity</strong>. If networks only used linear math (multiplying weights and adding biases), stacking 100 layers would mathematically collapse into a single straight-line equation. Linear equations cannot map complex realities like images or language. Non-linearity lets the network bend and warp to fit the data." },
    { question: "What is the Vanishing Gradient Problem?", answer: "In very deep networks (especially older RNNs), calculating gradients via backpropagation involves multiplying many small decimal numbers together (chain rule). If you multiply 0.1 * 0.1 * 0.1 repeatedly, the number rapidly shrinks to 0. Consequently, the weights in the earliest layers receive a gradient of 0 and literally stop learning. This prevented researchers from building deep networks for over a decade." },
    { question: "Why is ReLU (Rectified Linear Unit) the most popular activation function?", answer: "ReLU outputs the input directly if it is positive, and exactly 0 if it is negative (<code>max(0, x)</code>). \n1. It is incredibly cheap to compute mathematically compared to exponentials in Sigmoid.\n2. Because its derivative for positive numbers is exactly 1 (not a decimal), it completely solves the Vanishing Gradient problem, allowing for the training of massively deep networks." },
    { question: "What is the difference between an Epoch, a Batch, and an Iteration?", answer: "<strong>Epoch:</strong> One complete pass of the <em>entire</em> training dataset through the neural network.\n<strong>Batch (or Mini-Batch):</strong> Since datasets are too large to fit in GPU RAM, data is split into chunks (e.g., 32 images). This chunk is a Batch.\n<strong>Iteration:</strong> The number of batches needed to complete one Epoch. (e.g., 1000 images / Batch size of 100 = 10 Iterations per Epoch)." },
    { question: "What is a Convolutional Neural Network (CNN)?", answer: "A specialized deep learning architecture explicitly designed for grid-like data, such as images. Instead of using standard flat 'fully-connected' layers, CNNs slide small mathematical grids called 'Filters' or 'Kernels' across an image to detect spatial patterns like edges, corners, and eventually complex shapes like eyes or wheels, all while preserving the 2D spatial relationships of pixels." },
    { question: "Explain 'Max Pooling' in CNNs.", answer: "A downsampling operation used after a convolution. It slides a window (e.g., 2x2) over the feature map and only keeps the maximum value in that window, discarding the rest. This drastically reduces the computational size of the image, reduces memory usage, and provides 'translation invariance' (allowing the network to recognize an eye even if it shifts a few pixels to the left)." },
    { question: "What is Dropout?", answer: "An aggressive regularization technique to prevent neural networks from overfitting. During training, at every batch iteration, Dropout randomly 'turns off' (sets to zero) a percentage of neurons in a layer (e.g., 50%). This forces the network to not rely on any single neuron or specific pathway, forcing the <em>entire</em> network to learn redundant, generalized representations of the data." },
    { question: "What is the Softmax function?", answer: "Typically used as the final activation function in a multi-class classification network. It takes a vector of raw predicted numbers (logits) and mathematically squashes them so they range between 0 and 1, and crucially, <strong>sum to exactly 1.0</strong>. This converts raw network outputs into clean, easily interpreted probabilities (e.g., 90% Dog, 8% Cat, 2% Bird)." },
    { question: "What is the difference between Local Minima and Global Minima?", answer: "The 'Loss Landscape' of a deep neural network is like a massive, jagged mountain range. The goal of gradient descent is to walk down to the lowest possible point. A <strong>Local Minima</strong> is a small valley halfway down the mountain; the gradient is zero, so the network stops learning, mistakenly thinking it found the bottom. The <strong>Global Minima</strong> is the absolute lowest point on the entire map—the perfect, optimal configuration of weights." },
    { question: "What is Batch Normalization?", answer: "A technique that normalizes the inputs of each hidden layer <em>during</em> training (bringing their mean to 0 and variance to 1). This prevents 'Internal Covariate Shift'—where the distribution of data wildly changes as it passes through the layers. Batch Norm allows networks to train significantly faster and use much higher learning rates without destabilizing." },
    { question: "Explain Recurrent Neural Networks (RNNs) and their primary flaw.", answer: "RNNs were the old standard for sequential data (like text or stock prices). They process data one step at a time, maintaining an internal 'hidden state' (memory) that is passed to the next step. \n<strong>Flaw:</strong> Because they process sequentially, they cannot be parallelized on modern GPUs (making them horribly slow), and they suffer massive Vanishing Gradients, making them 'forget' the beginning of long sentences." },
    { question: "What are LSTMs and GRUs?", answer: "Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) are advanced variants of RNNs designed to solve the short-term memory limitation. They introduce complex mathematical 'Gates' (Forget, Input, Output gates) that explicitly learn what information is important enough to keep in memory for the long term, and what useless information should be flushed." },
    { question: "What is Transfer Learning?", answer: "A game-changing technique where a massive neural network is pre-trained by Google or Meta on millions of images (like ImageNet) to learn general features (edges, shapes). You then download this pre-trained model, freeze its core layers, replace only the final output layer, and 'fine-tune' it on your tiny dataset (e.g., classifying X-Rays). It allows you to achieve world-class accuracy with only 500 images instead of 5 million." },
    { question: "What is an Autoencoder?", answer: "An unsupervised neural network consisting of an Encoder and a Decoder. The Encoder compresses the input data (like a 1080p image) into a tiny, dense 'latent space' vector (a massive dimensionality reduction). The Decoder then attempts to reconstruct the original 1080p image solely from that tiny vector. Used for extreme data compression, denoising images, and feature extraction." },
    { question: "Explain Generative Adversarial Networks (GANs).", answer: "A brilliant architecture featuring two networks fighting each other. The <strong>Generator</strong> creates fake images from random noise. The <strong>Discriminator</strong> acts as an art critic, looking at both real images and the fake images, trying to guess which is which. As they train against each other, the Generator gets insanely good at forging perfect fakes, while the Discriminator gets insanely good at catching flaws. Result: photorealistic synthetic data." },
    { question: "What is the 'Learning Rate'?", answer: "The most critical hyperparameter in deep learning. It controls the 'step size' the optimizer takes when updating weights during gradient descent. \n- <strong>Too large:</strong> It overshoots the valley bottom and wildly destabilizes (blows up).\n- <strong>Too small:</strong> It takes millions of tiny steps, training incredibly slowly and likely getting permanently stuck in a sub-optimal local minima." },
    { question: "What is Data Augmentation?", answer: "A technique used to artificially expand a dataset, primarily in Computer Vision, to prevent overfitting. If you have 1,000 pictures of cats, you can dynamically rotate them 15 degrees, zoom in, flip them horizontally, and alter the brightness during training. To the neural network, this looks like 10,000 completely unique images, forcing it to learn what a cat is regardless of angle or lighting." },
    { question: "What is the difference between 'Parameters' and 'Hyperparameters'?", answer: "<strong>Parameters (Weights & Biases):</strong> The internal variables established during training. The algorithm learns and updates these automatically via backpropagation (e.g., a 7B parameter LLM).\n<strong>Hyperparameters:</strong> The 'settings' chosen by the human engineer <em>before</em> training begins. The algorithm cannot change these. (e.g., Learning Rate, Number of epochs, Batch Size, Network depth)." },
    { question: "Why do we use Cross-Entropy Loss for classification instead of Mean Squared Error (MSE)?", answer: "MSE assumes a continuous, linear output (like predicting a price). Classification relies on probabilities (Softmax). If a model predicts '0.99 Dog' but it's actually a Cat, MSE calculates a small squared error. Cross-Entropy uses logarithms to heavily scale the penalty when the model is confidently wrong. This log-scale explosion of error forces a massive gradient update, correcting the model drastically faster than MSE." }
];
