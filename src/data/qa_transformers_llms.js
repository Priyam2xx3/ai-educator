export const transformerQuestions = [
    { question: "What is the core issue with Recurrent Neural Networks (RNNs) that led to Transformers?", answer: "RNNs process data sequentially (word by word), maintaining a hidden state. Because of this inherently sequential design, they cannot be parallelized across GPUs, making them incredibly slow to train. Furthermore, passing an error backwards through a 1,000-word sentence causes massive Vanishing Gradients, severely limiting their long-term memory." },
    { question: "What was the revolutionary feature introduced in 'Attention Is All You Need'?", answer: "The complete removal of recurrence (loops) and replacing it solely with the <strong>Self-Attention</strong> mechanism. This allowed the model to process every single word in a 10,000-word document simultaneously (in parallel), creating a massive matrix of relationships where every word actively calculated its contextual relevance to every other word regardless of distance." },
    { question: "Explain the Query (Q), Key (K), and Value (V) paradigm in Self-Attention.", answer: "Borrowed from database retrieval systems:\n- <strong>Query (Q):</strong> 'What information am I looking for right now? (e.g., The word 'bank' seeking context).\n- <strong>Key (K):</strong> 'What information do I represent?' (e.g., The word 'river' broadcasting its identity).\n- <strong>Value (V):</strong> 'The actual meaning I carry.'\nIf Q strongly aligns mathematically with K, you multiply the result by V, meaning the word 'bank' now permanently holds the contextual meaning of a 'river', ignoring 'money'." },
    { question: "How does a Transformer 'know' word order if it processes everything at exactly the same time?", answer: "Since all words enter the model simultaneously, a scrambled sentence looks mathematically identical to an ordered one. To fix this, Transformers add <strong>Positional Encodings</strong> (specifically, sine and cosine mathematical waves of varying frequencies) directly into the word's input vector. This permanently stamps every single word's mathematical representation with its exact 1D location in the sequence." },
    { question: "What is Multi-Head Attention, and why is it superior to Single-Head Attention?", answer: "Instead of doing the massive Q, K, V matrix math once, Multi-Head Attention splits the calculations into multiple smaller 'heads' (e.g., 8 or 12 heads) running in parallel. Each head learns to focus on entirely different relationships—Head 1 might focus on grammar/syntax, Head 2 on pronoun assignment ('he' relates to 'John'), and Head 3 on emotional tone. Their results are concatenated at the end." },
    { question: "Explain the 'Encoder' block in the original Transformer.", answer: "The Encoder takes an input sequence and maps it into a continuous, extremely dense mathematical representation holding all contextual information. It uses Bidirectional Self-Attention, meaning every word looks at both the words before AND after it simultaneously to gain perfect context. Famous example: BERT (Bidirectional Encoder Representations from Transformers)." },
    { question: "Explain the 'Decoder' block in the original Transformer.", answer: "The Decoder takes the dense mathematical context generated by the Encoder and uses it to generate an entirely new sequence (like a French translation of an English input) one word at a time. It uses Masked Self-Attention, meaning when predicting word #5, it is strictly forbidden from 'peeking' at word #6. Famous examples: GPT series (Generative Pre-trained Transformer) which are Decoder-Only models." },
    { question: "Why is the Softmax function crucial inside the Scaled Dot-Product Attention equation?", answer: "When you multiply Q and K, you get raw, unbounded 'Attention Scores'. If one word has a score of 15,000 and another has 2, it's mathematically unstable. Softmax normalizes this entire vector into a probability distribution between 0 and 1, ensuring the total 'Attention' paid across the sentence always adds up exactly to 1.0 (100%)." },
    { question: "What does the equation <code>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V</code> represent?", answer: "This is the mathematical formula for Scaled Dot-Product Attention. You calculate the dot product of Q multiplied by the Transposed K matrix to get raw similarities. Crucially, you divide by the square root of the dimension size (<code>sqrt(d_k)</code>) to prevent the raw scores from growing so enormous that the Softmax function gets 'pushed to the edges', which would completely kill the gradient during backpropagation. Finally, you multiply by V to extract the meaning." },
    { question: "What role does the Feed-Forward Neural Network play directly after the Attention mechanism?", answer: "Self-Attention is brilliant for calculating relationships between words, but it is ultimately just a linear mixing of values. You still need massive, complex non-linearity to process that context. Every layer of a Transformer passes the Attention output through a standard dense, non-linear Feed-Forward network (usually with a ReLU activation) individually for every single token position, deeply processing the new combined meaning before passing it to the next layer." },
    { question: "What is Layer Normalization, and why is it preferred over Batch Normalization in NLP?", answer: "Batch Norm calculates statistics across the 'Batch' dimension (e.g., across 32 different sentences). This fails spectacularly in NLP because sentences are wildly different lengths. Layer Norm calculates statistics across the 'Feature' dimension (e.g., across the single 512-dimensional vector of one specific word in one specific sentence), completely ignoring batch size and sequence length constraints." },
    { question: "What are Residual Connections (Skip Connections), and why are they everywhere in Transformers?", answer: "If a network is 96 layers deep, the input signal and the gradients will completely die before reaching the end. A Residual Connection takes the exact input of a layer and mathematically <em>adds</em> it to the output of that layer (<code>Output = F(x) + x</code>). This provides an 'express highway' for raw mathematical signals and gradients to travel directly from the beginning of the network to the very end instantly, allowing models to scale to hundreds of layers without collapsing." },
    { question: "What is Masked Self-Attention?", answer: "Used primarily in Decoders. When training an Autoregressive model to predict the future, you must obviously prevent it from 'looking' at the future. Masked Attention applies a lower-triangular matrix of negative infinity (`-inf`) to all words located ahead of the current word. When passed through Softmax, `-inf` becomes `0.0`, meaning zero attention is mathematically possible towards future answers." },
    { question: "Why are Transformers fundamentally quadratic <code>O(N^2)</code> in memory complexity?", answer: "Because of the Self-Attention matrix. Every single word must calculate a dot product with every other word. For a 1,000 token sequence, that's a 1,000 x 1,000 attention grid (1,000,000 connections). If you double the sequence to 2,000, the grid explodes to 4,000,000 connections. This quadratic explosion caused the infamous 'Context Window' limitation in early LLMs." },
    { question: "What is FlashAttention?", answer: "A revolutionary, hardware-aware mathematical algorithm invented in 2022 that fundamentally reshaped Transformer processing. It realized that the massive bottleneck in Attention wasn't math, but copying massive matrices back and forth between GPU HBM memory and SRAM chips. FlashAttention calculates the exact same exact Softmax equations perfectly, but 'tiles' the math perfectly to fit in SRAM, speeding up Attention by 2-4x and allowing Context Windows to hit 100k+." },
    { question: "Explain the concept of an 'Embedding Vector' before it hits the Transformer.", answer: "Neural networks cannot read the text string 'Apple'. We must map the word to a unique mathematical coordinate in a dense, high-dimensional space (e.g., 768 dimensions for BERT). In this space, the vector (coordinates) for 'Apple' and 'Banana' are grouped closely together, while 'Bicycle' is far away. The Transformer inputs these geometric coordinates, not the text itself." },
    { question: "What is Byte-Pair Encoding (BPE) Tokenization?", answer: "Transformers rarely ingest entire words ('happily'). That creates a dictionary too large to fit in RAM. They also don't ingest single letters ('h','a','p'...), which makes sequences too long. BPE is a sub-word algorithm that statistically calculates the most common syllables in a language. 'happily' is split into two tokens: <code>['happi', 'ly']</code>. This creates a highly efficient, compressed vocabulary of ~50,000-100,000 sub-words, solving the 'Out of Vocabulary' (OOV) problem entirely." },
    { question: "How does BERT differ architecturally from GPT?", answer: "BERT (Bidirectional Encoder Representations from Transformers) is an <strong>Encoder-Only</strong> architecture. It replaces 15% of the words in a sentence with a `[MASK]` token, and the model is forced to use the complete surrounding context to guess the blank. Excellent for classification. \nGPT is a <strong>Decoder-Only</strong> architecture. It uses strict Autoregressive Masking, forced to always predict the next future token. Extremely bad at classification, but the absolute king of text generation." },
    { question: "What is the Cross-Attention mechanism in an Encoder-Decoder model?", answer: "In models like T5 or original Translation architectures, the Encoder reads the English sentence ('The dog'). The Decoder is trying to output French ('Le chien'). The Decoder performs Self-Attention on what it has typed so far ('Le'), but then performs a massive <strong>Cross-Attention</strong> step, sending its Queries to the Encoder's final Keys/Values, continually referencing the English source while generating." },
    { question: "Why do we add, rather than concatenate, Positional Encodings to Word Embeddings?", answer: "Adding two massive 768-dimensional vectors together sounds like it would destroy the information, but neural networks inherently operate in spaces with hundreds of dimensions. Because the Word Embeddings occupy a specific subspace, and the Positional Encodings occupy mathematically orthogonal frequencies, adding them allows the model to instantly differentiate both word MEANING and word LOCATION from a single, un-bloated vector." },
    { question: "Explain 'Weight Tying' in Transformers.", answer: "In many models, the massive weight matrix used to convert the final hidden states back into the 50,000 vocabulary probabilities (the unembedding layer) is often forced to physically share the exact same weights as the initial Input Embedding layer predicting the tokens. Since finding a word to generate is mathematically the inverse of embedding a word, sharing parameters saves massive amounts of GPU memory and acts as extreme regularization." }
];

export const llmQuestions = [
    { question: "What exactly makes a Language Model 'Large'?", answer: "Primarily Parameter Count and Dataset Size. Early NLP models had a few million parameters trained on megabytes of text. A modern LLM (like GPT-4 or Llama 3 70B) contains tens or hundreds of billions of individual neural weights, trained on petabytes of data—effectively the entirety of public human knowledge on the internet. This massive scale unlocks 'Emergent Abilities' (like reasoning logic or coding) not programmed directly." },
    { question: "What is 'Autoregressive Next-Token Prediction'?", answer: "The fundamental objective function underlying all generative LLMs. They are not trained to 'answer questions'. They are trained purely on the mathematical objective of taking a sequence of tokens and predicting the most statistically probable single next token. Doing this accurately across trillions of web pages forces the model to inherently learn grammar, facts, and logical structure. <code>The cat sat on the ___ -> 'mat'</code>." },
    { question: "What is the difference between a Foundation (Base) model and an Instruct/Chat model?", answer: "A <strong>Base Model</strong> is only trained on raw internet text completion. If prompted with 'What is the capital of France?', it might predict the next words to be 'What is the capital of Spain?', simply mimicking a geography quiz list.\nAn <strong>Instruct Model</strong> takes that base model and fine-tunes it extensively on 'Helpful Assistant' conversational formatting (User: Question -> Assistant: Answer), forcing it to actually execute commands." },
    { question: "What is Supervised Fine-Tuning (SFT) in the context of LLMs?", answer: "The first step of turning a Base Model into a Chatbot. Researchers create 10,000 - 100,000 highly curated, perfect human-written conversational pairs demonstrating how a polite, intelligent assistant should act. The LLM's weights are slightly updated to learn this specific dialogue structure and formatting, largely stopping it from endlessly spewing random internet text." },
    { question: "What is RLHF (Reinforcement Learning from Human Feedback)?", answer: "The most crucial step in modern LLM training. 1) Ask the model a question. 2) Make it generate 4 different answers. 3) Pay a human to rank them from best to worst. 4) Train a 'Reward Model' (another neural network) on those human preferences. 5) Use the PPO (Proximal Policy Optimization) algorithm to iteratively force the massive LLM to generate outputs that maximize the Reward Model's score. This forces the model to learn fuzzy human concepts like 'safety', 'politeness', and 'helpfulness'." },
    { question: "What is LoRA (Low-Rank Adaptation)?", answer: "A massive breakthrough in Parameter Efficient Fine-Tuning (PEFT). Fine-tuning a 70B parameter model requires terrabytes of VRAM. <strong>LoRA freezes the entire massive 70B model entirely.</strong> Instead, it injects tiny, highly compressed mathematical matrices (Rank-Decomposed matrices) into the Attention layers. Only these tiny matrices are trained (often <0.1% of total parameters). You can then merge them back. This allows individuals to fine-tune massive models on cheap, consumer gaming GPUs." },
    { question: "How does Quantization (e.g., INT8, INT4) allow massive LLMs to run on smaller computers?", answer: "Neural networks normally use ultra-precise 32-bit float (FP32) decimals for their weights (e.g., 0.12345678). A 70B model at FP32 takes ~280 Gigabytes of VRAM. Quantization brutally rounds these massive decimals down to simple 8-bit (INT8) or 4-bit (INT4) chunks (e.g., rounding 0.123... down to just 0.1). It causes only a minuscule 1-2% drop in 'smartness', but completely shatters the memory requirement, allowing a 70B model to run on a 36GB Mac." },
    { question: "What is the 'Temperature' parameter during generation?", answer: "Prior to the final Softmax step, you can divide the raw logits by a Temperature value to alter the probability 'curve'. \n- <strong>T=0.1 (Low):</strong> The curve becomes immensely sharp, forcing the model to almost always pick the most mathematically obvious word (highly deterministic, great for coding).\n- <strong>T=0.9 (High):</strong> The curve flattens out, giving lower-probability words a fighting chance to be selected (highly creative, prone to hallucination, great for poetry)." },
    { question: "Explain the difference between Top-K and Top-p (Nucleus) Sampling.", answer: "Both prevent the model from picking wildly wrong 'tail-end' words.\n- <strong>Top-K:</strong> Hard cutoff. Sort all vocabulary by probability, and the model is ONLY allowed to pick from the Top 50 words, completely deleting the rest. \n- <strong>Top-p (Nucleus):</strong> Dynamic cutoff. Sort words by probability. The model scans down the list until their combined probabilities sum up to the chosen decimal (e.g., p=0.9 or 90%). Some turns this is 2 words, some turns it is 100 words. Vastly superior." },
    { question: "What causes a model to 'Hallucinate'?", answer: "LLMs do not have an internal database of truth. They are mathematical prediction engines. If you ask a highly specific, obscure question it never saw in training, the Autoregressive engine cannot mathematically stop itself. It is forced to generate the next structurally probable word based on grammar—resulting in a perfectly formatted, highly confident, utterly fabricated lie." },
    { question: "Explain the concept of 'In-Context Learning' (Prompting).", answer: "A mysterious emergent ability in very massive LLMs. Without changing a single neural weight (no fine-tuning), you can teach the model a brand new skill (like a fake programming language or a secret code) purely by placing 3-4 examples of it inside the text prompt itself (Few-Shot Prompting). The model dynamically rewires its Attention heads to 'learn' the pattern on the fly during generation." },
    { question: "What is Chain-of-Thought (CoT) Prompting, and why is it so effective?", answer: "Adding the magic phrase <em>'Let's think step by step'</em> to a prompt. LLMs process data linearly. For complex math, if they try to guess the final answer instantly in one jump (one token prediction), they usually fail. By forcing them to spell out their logic word by word, the model gives itself vastly more computational space and 'reads' its own logic as it goes, dramatically increasing accuracy on logic puzzles." },
    { question: "What is a MoE (Mixture of Experts) Architecture?", answer: "A massive scaling trick (famously used in GPT-4 and Mistral). Instead of having one giant dense 70-Billion parameter layer that fires completely for every word, MoE replaces it with 8 entirely distinct smaller 'Expert' networks. A Router network looks at the token ('dog') and routes it to only the 2 most relevant experts. This allows you to build a massive, heavily knowledgeable 47B model that runs with the sheer computational speed of a tiny 12B model." },
    { question: "Why is an LLM's 'Context Window' a massive technical hurdle?", answer: "Because Self-Attention math scales quadratically (O(N^2)). A 4,000-token context window is fast. An 128,000-token context window requires exponentially massive amounts of VRAM just to store the Key-Value Cache (KV Cache) matrix. Engineers must rely on hardware tricks like FlashAttention or architectural tricks like Ring Attention to make massive context windows mathematically feasible." },
    { question: "What is the KV-Cache (Key Value Cache) during inference?", answer: "When generating text word-by-word (Autoregressive), to predict word #100, the model normally has to perfectly recalculate the Q,K,V vectors for words 1 through 99. This is horribly slow. To fix this, after computing word #1's Keys and Values, the GPU permanently saves them in VRAM. It does this for every word. To generate word #100, it ONLY calculates the Q,K,V of word #100, and mathematically multiplies it against the massive saved cache of the past 99 words." },
    { question: "If a model starts repeating itself in an infinite loop, what generation parameter is likely missing or poorly tuned?", answer: "<strong>Repetition Penalty (Presence Penalty/Frequency Penalty).</strong> As a model repeats a word ('The The The'), the word gains mathematical dominance in the context window, drastically increasing the chance it is picked again. A Repetition Penalty applies a strict mathematical multiplier reduction to the probability of any token that already physically exists in the generated text, physically preventing the loop." },
    { question: "What is InstructPix2Pix or Multimodal LLMs?", answer: "A massive expansion where the LLM is not just fed text Embeddings, but also heavily compressed image vectors from a Vision Encoder (like CLIP). The Transformer is modified to perform Cross-Attention against the image matrix. This allows you to prompt the LLM with an image of a broken refrigerator and ask, 'What part is broken here?'" },
    { question: "What is DPO (Direct Preference Optimization)?", answer: "A massive, recent replacement for the highly complex, unstable PPO algorithms used in RLHF. DPO bypasses the need to build a separate 'Reward Model' entirely. It mathematically manipulates the loss function to directly update the LLM's weights based purely on pairs of ('Good Answer', 'Bad Answer'), making alignment training vastly cheaper, faster, and more stable." },
    { question: "What are 'System Prompts' (or Developer Messages)?", answer: "A foundational meta-instruction injected at the very top of the prompt sequence. Because it sits chronologically first and often has heightened internal weight during training, it massively guides the persona and restrictions of the model. <em>'You are a strict math teacher. Never give the answer directly, only give hints.'</em> The model uses this directive rigidly to govern all subsequent user interactions." },
    { question: "Explain the concept of 'Grokking' in LLMs.", answer: "A fascinating phenomenon noticed during training. An AI will repeatedly hit 0% accuracy on validation data for thousands of epochs, appearing totally stuck while overfitting heavily on the training data. Then, completely instantly and without warning, the validation accuracy shoots vertically to 100%. The model suddenly 'groks' (understands the true underlying algorithm) instead of just memorizing the data." },
    { question: "How does Rotary Positional Embeddings (RoPE) improve upon original Transformers?", answer: "The original 2017 paper added static sine/cosine values to embeddings (Absolute Encoding). This is flawed because language relies heavily on relative distance (Word A is exactly 3 words away from Word B), not absolute position (Word A is at position #437). RoPE physically rotates the high-dimensional embedding vectors by an angle proportional to their position, brilliantly preserving mathematically perfect relative distance regardless of sequence length." }
];
