<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers | AI Educator</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="app-container">
        <!-- Navbar -->
        <nav class="navbar">
            <div class="logo">‚ú¶ AI<span>Educator</span></div>
            <div class="nav-actions">
                <a href="index.html" class="nav-link">Home</a>
                <a href="roadmap.html" class="btn btn-outline" style="padding: 0.5rem 1rem;">Roadmap</a>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">üåô</button>
                <button id="mobile-menu-btn" class="mobile-menu-btn">‚ò∞</button>
            </div>
        </nav>

        <!-- Main Layout -->
        <div class="main-content-layout">
            <!-- Sidebar -->
            <aside id="sidebar" class="sidebar">
                <div class="nav-links">
                    <a href="index.html">üè† Home</a>
                    <a href="roadmap.html">üìç Roadmap</a>
                    <a href="ml.html">üìò Machine Learning</a>
                    <a href="dl.html">üìó Deep Learning</a>
                    <a href="transformers.html" class="active">üìï Transformers</a>
                    <a href="llm.html">üìô LLMs</a>
                    <a href="rag.html">üìí RAG</a>
                    <a href="agents.html">üìì AI Agents</a>
                    <a href="algorithms.html">üìë Algorithms</a>
                    <a href="references.html">üìé References</a>
                </div>
            </aside>

            <!-- Page Content -->
            <main class="content-area">
                <h1 class="page-header delay-1">Transformers Architecture</h1>

                <section class="glass delay-1" style="padding: 2rem; margin-bottom: 2rem;">
                    <h2>The "Attention is All You Need" Revolution</h2>
                    <p>Introduced by Google in 2017, the Transformer architecture revolutionized Natural Language
                        Processing by dispensing with recurrence (RNNs/LSTMs) entirely, relying solely on an
                        <strong>Attention Mechanism</strong> to draw global dependencies between input and output.
                    </p>
                </section>

                <div class="accordion delay-2">
                    <div class="accordion-item">
                        <div class="accordion-header">
                            Self-Attention Mechanism
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>What is it?</strong> A mechanism allowing the model to look at other words
                                    in the input sequence to gain a better understanding of the current word. For
                                    example, knowing that "it" refers to the "animal" in the sentence "The animal didn't
                                    cross the street because <em>it</em> was too tired."</li>
                                <li><strong>Query, Key, Value:</strong> Mapped concepts from retreival systems. A word's
                                    Query vector is scored against every other word's Key vector, yielding a weight used
                                    to sum the Value vectors.</li>
                                <li><strong>Multi-Head Attention:</strong> Running the attention mechanism multiple
                                    times in parallel to capture different types of relationships (e.g., grammatical vs
                                    semantic).</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Encoder-Decoder Architecture
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Encoder:</strong> Processes the input sequence into a continuous
                                    representation that holds all the learned information. Composed of Self-Attention
                                    and Feed-Forward Neural Networks.</li>
                                <li><strong>Decoder:</strong> Takes the Encoder output and autoregressively generates
                                    the final output sequence, one token at a time. It uses Masked Self-Attention to
                                    prevent looking at future tokens during training.</li>
                                <li><strong>Applications:</strong> Used primarily for sequence-to-sequence tasks like
                                    Machine Translation (e.g., translating English to French).</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            BERT (Encoder Only)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>What is BERT?</strong> Bidirectional Encoder Representations from
                                    Transformers. A model developed by Google.</li>
                                <li><strong>Architecture:</strong> It utilizes only the Encoder block of the
                                    Transformer.</li>
                                <li><strong>Bidirectional:</strong> Unlike previous models, it looks at text from
                                    left-to-right AND right-to-left simultaneously natively.</li>
                                <li><strong>Use Cases:</strong> Perfect for understanding language context, such as text
                                    classification, sentiment analysis, and answering questions extracted from text.
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            GPT (Decoder Only)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>What is GPT?</strong> Generative Pre-trained Transformer. A series of models
                                    developed by OpenAI.</li>
                                <li><strong>Architecture:</strong> It utilizes only the Decoder block of the
                                    Transformer.</li>
                                <li><strong>Autoregressive:</strong> It predicts the next token in a sequence based only
                                    on the previous tokens (left-to-right).</li>
                                <li><strong>Use Cases:</strong> Incredible at generating coherent, contextual text.
                                    Powers conversational AI like ChatGPT.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Deep Dive Q&A (A Curious Student's Treasure)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li>
                                    <strong>Q: Before Transformers, how did AI read text, and why was it
                                        bad?</strong><br>
                                    <span class="text-muted">A: AI used Recurrent Neural Networks (RNNs) which read
                                        words sequentially, like a human reading left-to-right. If a paragraph was 500
                                        words long, by the time the RNN reached word 500, it had "forgotten" the context
                                        of word 1. It was also incredibly slow to train because you couldn't process
                                        word 50 until you finished word 49.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Why are Transformers faster to train than RNNs?</strong><br>
                                    <span class="text-muted">A: Transformers do not read sequentially; they read the
                                        *entire sentence simultaneously* in parallel using matrix multiplication. This
                                        means they can take full advantage of modern GPUs, reducing training time from
                                        months to days.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: If Transformers read everything at once, how do they know the order of
                                        words? ("Dog bites man" vs "Man bites dog")</strong><br>
                                    <span class="text-muted">A: This is where <strong>Positional Encoding</strong> comes
                                        in. Since the model has no concept of sequential time, we inject a mathematical
                                        signature (using sine and cosine waves) into every word's embedding before it
                                        enters the model. This acts like a "timestamp" so the model knows "Dog" is at
                                        position 1 and "Man" is at position 3.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Explain the abstract formula: `Attention(Q, K, V) = softmax(QK^T /
                                        sqrt(d_k))V` without the jargon.</strong><br>
                                    <span class="text-muted">A: Imagine you are at a library searching for a specific
                                        book on physics. You have a search query (<strong>Query, Q</strong>). The
                                        library's catalog system has titles for every book (<strong>Keys, K</strong>).
                                        You compare your Query against all the Keys (QK^T) to get a "match score" of how
                                        relevant each book is. You then turn those scores into percentages using
                                        Softmax. Finally, you extract the actual content of the books (<strong>Values,
                                            V</strong>), giving the most attention (multiply by percentage) to the books
                                        that matched best.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Why do we divide by the square root of dimensions (`sqrt(d_k)`) in that
                                        formula?</strong><br>
                                    <span class="text-muted">A: When multiplying large vectors (Q and K), the resulting
                                        numbers can become massively large. When we pass those massive numbers into the
                                        Softmax function, it outputs a `1` for the largest number and `0` for everything
                                        else, making the gradient flat and killing the learning process. Dividing by
                                        `sqrt(d_k)` scales the numbers down to keep gradients stable.</span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </main>
        </div>

        <button id="backToTop" title="Go to top">‚Üë</button>
    </div>

    <script src="script.js"></script>
</body>

</html>