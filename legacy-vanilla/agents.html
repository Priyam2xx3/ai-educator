<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Agents | AI Educator</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="app-container">
        <!-- Navbar -->
        <nav class="navbar">
            <div class="logo">‚ú¶ AI<span>Educator</span></div>
            <div class="nav-actions">
                <a href="index.html" class="nav-link">Home</a>
                <a href="roadmap.html" class="btn btn-outline" style="padding: 0.5rem 1rem;">Roadmap</a>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">üåô</button>
                <button id="mobile-menu-btn" class="mobile-menu-btn">‚ò∞</button>
            </div>
        </nav>

        <!-- Main Layout -->
        <div class="main-content-layout">
            <!-- Sidebar -->
            <aside id="sidebar" class="sidebar">
                <div class="nav-links">
                    <a href="index.html">üè† Home</a>
                    <a href="roadmap.html">üìç Roadmap</a>
                    <a href="ml.html">üìò Machine Learning</a>
                    <a href="dl.html">üìó Deep Learning</a>
                    <a href="transformers.html">üìï Transformers</a>
                    <a href="llm.html">üìô LLMs</a>
                    <a href="rag.html">üìí RAG</a>
                    <a href="agents.html" class="active">üìì AI Agents</a>
                    <a href="algorithms.html">üìë Algorithms</a>
                    <a href="references.html">üìé References</a>
                </div>
            </aside>

            <!-- Page Content -->
            <main class="content-area">
                <h1 class="page-header delay-1">Autonomous AI Agents</h1>

                <section class="glass delay-1" style="padding: 2rem; margin-bottom: 2rem;">
                    <h2>What are AI Agents?</h2>
                    <p>An AI agent is an autonomous system powered by an LLM as its "brain," capable of planning,
                        holding memory, and invoking external tools to complete complex, multi-step tasks in the real
                        world.</p>
                </section>

                <div class="accordion delay-2">
                    <div class="accordion-item">
                        <div class="accordion-header">
                            Core Architecture
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Brain (LLM):</strong> The foundational model providing reasoning and logic
                                    capabilities. It processes the prompt and plans the next moves.</li>
                                <li><strong>Perception/Input:</strong> How the agent receives data (text instructions,
                                    vision from screenshots, code logs).</li>
                                <li><strong>Action/Output:</strong> How the agent interacts with its environment
                                    (clicking buttons, executing code, pushing to GitHub, calling APIs).</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Key Capabilities
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Tools (Function Calling):</strong> An LLM alone only predicts text. Tools
                                    allow it to interact with the world. Examples: Calculator, Web Search API, Python
                                    Interpreter, SQL Executor.</li>
                                <li><strong>Memory:</strong>
                                    <ul>
                                        <li><em>Short-term:</em> The current conversation history held within the
                                            context window.</li>
                                        <li><em>Long-term:</em> Saving experiences, user preferences, or parsed
                                            documents in a Vector Database for future retrieval.</li>
                                    </ul>
                                </li>
                                <li><strong>Planning (ReAct):</strong> Agents use prompting strategies like ReAct
                                    (Reasoning and Acting) to break a large goal into smaller steps: Thought ‚Üí Action ‚Üí
                                    Observation ‚Üí Thought.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Example Use Cases
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong> Devin (Software Engineering Agent):</strong> Give it a GitHub issue; it
                                    clones the repo, reads the code, sets up an environment, runs tests, writes the
                                    patch, and submits a PR.</li>
                                <li><strong> Financial Analyst Agent:</strong> Given a task "Analyze Apple's Q3
                                    earnings", the agent uses a web search tool to find the PDF report, uses a
                                    calculator tool to analyze margins, and writes a summary report.</li>
                                <li><strong> Multi-Agent Systems (e.g., AutoGen, CrewAI):</strong> Multiple agents with
                                    distinct personas (e.g., Code Writer, QA Tester, Product Manager) chatting and
                                    collaborating to build an app.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Practice & Frameworks
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>LangChain / LangGraph:</strong> Frameworks for orchestrating complex chains
                                    and cyclic graphs of agent actions.</li>
                                <li><strong>LlamaIndex:</strong> Perfect for data-heavy intelligent agents performing
                                    advanced RAG.</li>
                                <li><strong>Mini Project:</strong> Build a custom Research Agent in Python using an LLM
                                    API and the SerpAPI (Google Search) tool. Ask it a question, and watch it search,
                                    summarize, and cite the results.</li>
                            </ul>
                            <pre><code># ReAct Prompt Example Pattern
Question: "What is the capital of France, and what is the current weather there?"
Thought 1: I need to find the capital of France.
Action 1: WebSearch["Capital of France"]
Observation 1: Paris.
Thought 2: I need to find the weather in Paris.
Action 2: WeatherAPI["Paris"]
Observation 2: 15¬∞C and sunny.
Thought 3: I have all the information needed.
Final Answer: The capital of France is Paris, and the current weather is 15¬∞C and sunny.</code></pre>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Deep Dive Q&A (A Curious Student's Treasure)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li>
                                    <strong>Q: What is the defining difference between a basic LLM Chatbot (like
                                        ChatGPT) and an AI Agent?</strong><br>
                                    <span class="text-muted">A: A chatbot is reactive: it answers a single prompt and
                                        stops. An Agent is proactive and autonomous: given a high-level goal ("plan my
                                        vacation"), it will independently break the goal into subtasks, execute an
                                        action (search flights), observe the result, adjust its plan if the action
                                        failed, and loop iteratively until the final objective is achieved. </span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: How does an LLM actually "use a tool" or "call an API"?</strong><br>
                                    <span class="text-muted">A: Function calling! We give the LLM a text description of
                                        the available tools (e.g., `get_weather(city: string)`). When the user asks "How
                                        is the weather in Tokyo?", the LLM recognizes it needs that tool. Instead of
                                        generating an English response, the LLM outputs a specific JSON object:
                                        `{"name": "get_weather", "arguments": {"city": "Tokyo"}}`. Our background code
                                        parses that JSON, runs the actual Python code/API, gets the result (e.g., 20¬∞C),
                                        and sends that result *back* into the LLM as a new hidden prompt. The LLM then
                                        finally says, "It is 20¬∞C in Tokyo."</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Why do Agents sometimes get stuck in an "Infinite Loop"?</strong><br>
                                    <span class="text-muted">A: If an agent calls a tool (e.g., `web_search`) and gets
                                        an error from the server, a smart agent (like GPT-4) will read the error,
                                        realize it made a mistake, and try a different search term. A weaker agent (like
                                        a smaller open-source model) might lack the reasoning to fix the error, so it
                                        just tries the exact same broken search again, gets the same error, tries
                                        again... looping forever until forced to stop. Engineering robust error-handling
                                        prompts into the agent loop is critical.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Explain the value of "ReAct" prompting. Why is the "Thought" so
                                        important?</strong><br>
                                    <span class="text-muted">A: ReAct stands for Reasoning and Acting. If an LLM goes
                                        straight from the Question to the Action, it often acts impulsively or makes
                                        logic errors. By forcing the LLM to explicitly write out its internal monologue
                                        (Thought: "I need to find X before I can calculate Y..."), we give the neural
                                        network "computational space" to process the logic before it commits to an
                                        action. It dramatically reduces errors and improves complex task
                                        completion.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: How do Multi-Agent systems (like CrewAI or AutoGen) work, and why not
                                        just use one smart agent?</strong><br>
                                    <span class="text-muted">A: If you ask one agent to write code, test it, and review
                                        its security, it suffers from "role confusion" or agrees with its own mistakes.
                                        In a Multi-Agent system, you instantiate three separate agents with different
                                        system prompts: A Coder, a Tester, and a Security Auditor. The Coder writes the
                                        script and passes it to the Tester. The Tester runs it, finds a bug, and argues
                                        with the Coder to fix it. This adversarial collaboration mimics a real human
                                        company and produces significantly higher quality results.</span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="section-divider"></div>
            </main>
        </div>

        <button id="backToTop" title="Go to top">‚Üë</button>
    </div>

    <script src="script.js"></script>
</body>

</html>