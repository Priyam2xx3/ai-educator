<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning | AI Educator</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="app-container">
        <!-- Navbar -->
        <nav class="navbar">
            <div class="logo">‚ú¶ AI<span>Educator</span></div>
            <div class="nav-actions">
                <a href="index.html" class="nav-link">Home</a>
                <a href="roadmap.html" class="btn btn-outline" style="padding: 0.5rem 1rem;">Roadmap</a>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">üåô</button>
                <button id="mobile-menu-btn" class="mobile-menu-btn">‚ò∞</button>
            </div>
        </nav>

        <!-- Main Layout -->
        <div class="main-content-layout">
            <!-- Sidebar -->
            <aside id="sidebar" class="sidebar">
                <div class="nav-links">
                    <a href="index.html">üè† Home</a>
                    <a href="roadmap.html">üìç Roadmap</a>
                    <a href="ml.html">üìò Machine Learning</a>
                    <a href="dl.html" class="active">üìó Deep Learning</a>
                    <a href="transformers.html">üìï Transformers</a>
                    <a href="llm.html">üìô LLMs</a>
                    <a href="rag.html">üìí RAG</a>
                    <a href="agents.html">üìì AI Agents</a>
                    <a href="algorithms.html">üìë Algorithms</a>
                    <a href="references.html">üìé References</a>
                </div>
            </aside>

            <!-- Page Content -->
            <main class="content-area">
                <h1 class="page-header delay-1">Deep Learning Concepts</h1>

                <section class="glass delay-1" style="padding: 2rem; margin-bottom: 2rem;">
                    <h2>What is Deep Learning?</h2>
                    <p>Deep Learning is a specialized sub-field of Machine Learning based on Artificial Neural Networks
                        with multiple layers (hence "deep"). It excels at finding complex patterns in unstructured data
                        like images, text, and audio.</p>
                </section>

                <div class="accordion delay-2">
                    <div class="accordion-item">
                        <div class="accordion-header">
                            Core Fundamentals
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Neural Networks:</strong> Computing systems inspired by the biological
                                    neural networks of animal brains. Made up of layers of "neurons" (nodes).</li>
                                <li><strong>Activation Functions:</strong> Mathematical equations that determine the
                                    output of a neural network node (e.g., ReLU, Sigmoid, Tanh). They introduce
                                    non-linearity.</li>
                                <li><strong>Backpropagation:</strong> The core algorithm behind how neural networks
                                    learn. It calculates the gradient of the loss function to adjust weights and biases
                                    across all internal layers.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Architectures (Subpages inside the topic)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>ANN (Artificial Neural Networks):</strong> The standard multi-layer
                                    perceptron. Good for tabular data and general non-linear mapping.</li>
                                <li><strong>CNN (Convolutional Neural Networks):</strong> Specialized for grid-like data
                                    like Images. Uses Convolutional layers and pooling to automatically extract spatial
                                    features.</li>
                                <li><strong>RNN (Recurrent Neural Networks):</strong> Designed for sequential data
                                    (Text, Time-series). Includes loops to allow information to persist. Variants
                                    include LSTMs and GRUs.</li>
                                <li><strong>GNN (Graph Neural Networks):</strong> Operates on graph structures, useful
                                    for social networks, molecular structures, and recommendation systems.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Practice Section
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Image Classification:</strong> <span class="text-muted">Use a basic CNN to
                                        classify objects in the CIFAR-10 dataset into 10 distinct classes.</span></li>
                                <li><strong>MNIST Digit Recognition:</strong> <span class="text-muted">The "Hello World"
                                        of Deep Learning. Classify handwritten digits (0-9) using a dense neural
                                        network.</span></li>
                                <li><strong>Cat vs Dog Classifier:</strong> <span class="text-muted">Build a CNN and use
                                        data augmentation techniques to distinguish between cats and dogs.</span></li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Mini Projects
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li><strong>Face Mask Detection:</strong> <span class="text-muted">Train an object
                                        detection CNN, or fine-tune an existing model like MobileNet, to detect if
                                        people in a frame are wearing masks.</span></li>
                                <li><strong>Emotion Detection:</strong> <span class="text-muted">Analyze facial
                                        expressions in images to classify emotions (happy, sad, angry, surprised) using
                                        deep learning.</span></li>
                                <li><strong>Object Detection:</strong> <span class="text-muted">Implement a YOLO (You
                                        Only Look Once) architecture variant to identify and draw bounding boxes around
                                        objects in real-time video.</span></li>
                            </ul>
                        </div>
                    </div>

                    <div class="accordion-item">
                        <div class="accordion-header">
                            Deep Dive Q&A (A Curious Student's Treasure)
                            <span class="accordion-icon">‚ñº</span>
                        </div>
                        <div class="accordion-content">
                            <ul style="margin-top: 1rem;">
                                <li>
                                    <strong>Q: If I can use standard Machine Learning, why would I ever need Deep
                                        Learning?</strong><br>
                                    <span class="text-muted">A: Deep learning absolutely dominates when dealing with
                                        unstructured data: pixels in an image, words in a paragraph, or frequencies in
                                        an audio file. Feature engineering (manually telling the computer what lines or
                                        shapes look like) is nearly impossible for these formats. Neural Networks
                                        automatically discover these hierarchical features‚Äîlayer 1 finds edges, layer 2
                                        finds shapes, layer 3 finds faces‚Äîwithout human intervention.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: What exactly is an Activation Function and why does a Neural Network need
                                        it?</strong><br>
                                    <span class="text-muted">A: Without activation functions, a neural network is just a
                                        giant linear formula (like multiplying matrices). It wouldn't be able to solve
                                        complex, curved, non-linear problems. Activation functions (like ReLU or
                                        Sigmoid) act as the "gatekeepers"‚Äîthey decide whether a neuron should "fire" or
                                        stay dormant. They bend the mathematical space, allowing the network to wrap
                                        around wildly complex data manifolds.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: What is the "Vanishing Gradient" problem that plagued early Deep
                                        Learning?</strong><br>
                                    <span class="text-muted">A: Neural networks learn by passing error signals backwards
                                        from the output to the input (Backpropagation) using calculus chain rules
                                        (multiplying gradients). If you multiply fractions (like 0.5 * 0.5 * 0.5), the
                                        number quickly approaches zero. In a "deep" network, by the time the error
                                        signal reaches the first few layers, it "vanishes" to zero, meaning the early
                                        layers never learn anything! The invention of the ReLU activation function
                                        (which doesn't output fractions) largely solved this.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: I always hear about Epochs, Batches, and Iterations. What's the
                                        difference?</strong><br>
                                    <span class="text-muted">A: Imagine reading a 10,000-page book to study for an exam.
                                        You can't read it all at once.<br>- <strong>Batch Size:</strong> Reading 100
                                        pages at a time before testing yourself.<br>- <strong>Iteration:</strong> Every
                                        time you finish a 100-page batch.<br>- <strong>Epoch:</strong> Once you have
                                        read the entire 10,000-page book.<br>You usually need multiple Epochs (reading
                                        the whole book several times) to fully master the material!</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Why do Neural Networks "forget" things, and how does Dropout prevent
                                        it?</strong><br>
                                    <span class="text-muted">A: Networks love to lazily memorize training data
                                        (Overfitting) instead of learning generalizing rules. <strong>Dropout</strong>
                                        is a brutal but effective technique: during training, we randomly turn off a
                                        percentage of neurons (say, 20%). The network can no longer rely on any single
                                        neuron to memorize the answer. It is forced to learn robust, distributed
                                        representations across the whole network, making it much smarter on unseen
                                        data.</span>
                                </li>
                                <li style="margin-top: 1rem;">
                                    <strong>Q: Why are CNNs so much better at analyzing Images than standard multi-layer
                                        networks?</strong><br>
                                    <span class="text-muted">A: Standard networks treat an image as a single, long 1D
                                        list of pixels, destroying spatial geometry. A CNN (Convolutional Neural
                                        Network) uses a "sliding window" (a kernel) that moves over the image in 2D. It
                                        looks at local patches of pixels together, inherently understanding that pixels
                                        close to each other form cohesive shapes, lines, and textures.</span>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </main>
        </div>

        <button id="backToTop" title="Go to top">‚Üë</button>
    </div>

    <script src="script.js"></script>
</body>

</html>